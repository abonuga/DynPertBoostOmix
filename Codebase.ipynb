{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the notebook outlines a comprehensive pipeline to load, process, reshape, and prepare multi-modal time-series data for downstream analytical tasks. The pipeline integrates multiple datasets, merges them by `specimen_id` and `subject_id`, aligns them with temporal data from a clinical study (e.g., relative days to a booster vaccination), and ultimately constructs a 3D PyTorch tensor ready for machine learning workflows.\n",
    "\n",
    "#### Overview of the Steps\n",
    "\n",
    "1. **Data Loading:**\n",
    "   - **Input Files:**\n",
    "     - `TheData.xlsx`: Contains various sheets with biological measurements at different time points for subjects.\n",
    "     - `training_pbmc_gene_expression_wide_tpm.tsv`: TPM (Transcripts Per Million) gene expression data aligned by `specimen_id`.\n",
    "     - `genenames.tsv`: Contains the gene names or identifiers to filter the gene expression data.\n",
    "   \n",
    "   The code reads these files into pandas DataFrames for subsequent processing.\n",
    "\n",
    "2. **Data Filtering and Alignment:**\n",
    "   - **Gene Filtering:**  \n",
    "     Using `genenames.tsv`, we create a list of target genes and filter the TPM dataset to retain only these genes and the `specimen_id` column.\n",
    "   \n",
    "   - **Temporal Data Mapping:**  \n",
    "     From the `subject_specimen` sheet in `TheData.xlsx`, we extract timepoint information (`actual_day_relative_to_boost`) and create a mapping (`specimen_time_mapping`) to associate each `specimen_id` with its corresponding timepoint.\n",
    "\n",
    "3. **Pivoting to Time-Series Format:**\n",
    "   - Each relevant sheet (other than `subject_specimen`) is merged with the time mapping and pivoted into a time-series format. This results in DataFrames keyed by `subject_id` (rows) and timepoints (columns), with column names representing `feature_day<timepoint>`.\n",
    "\n",
    "4. **Baseline Consolidation:**\n",
    "   - Identification of baseline timepoints (e.g., those before day 0) is performed, and their values are consolidated into a single \"baseline\" (day0.0) feature by taking the median across all baseline measurements. This step is crucial when baseline values are spread across multiple negative timepoints.\n",
    "   \n",
    "   - A function `consolidate_baseline_features_handle_nan` is used to handle missing baseline data and ensure a consistent baseline reference point across subjects.\n",
    "\n",
    "5. **Chunked Processing of Large DataFrames:**\n",
    "   - Some DataFrames may be very large (tens of thousands of columns). The code includes a function `process_large_dataframe_in_chunks` to apply baseline consolidation in manageable pieces, avoiding memory issues.\n",
    "\n",
    "6. **Combining Processed DataFrames:**\n",
    "   - Once each individual DataFrame (gene expression, cellular frequency data, cytokine concentrations, antibody levels, etc.) has been processed, they are merged into a single `combined_dataframe` keyed by `subject_id`.\n",
    "   \n",
    "   - The final combined DataFrame contains a rich set of features (genes, proteins, cell frequencies) across multiple timepoints for each subject.\n",
    "\n",
    "7. **Feature and Time Parsing:**\n",
    "   - A helper function `parse_features_and_temporal_dimensions_refined` extracts the `feature_name` and `temporal_dimension` (timepoint) from column names. This parsing is necessary to later build structured tensors and ensure that each (feature, time) pair is identifiable.\n",
    "\n",
    "8. **Constructing a 3D PyTorch Tensor:**\n",
    "   - Using `create_3d_tensor_with_assertions`, we transform the combined DataFrame into a tensor of shape `[Number_of_Subjects, Number_of_Timepoints, Number_of_Features]`.\n",
    "   - Subjects are sorted and indexed, timepoints are sorted and indexed, and features are sorted and indexed. Each entry in the tensor corresponds to a data value (e.g., gene expression at a given time and subject).\n",
    "\n",
    "9. **Imputation via Cubic Spline Interpolation:**\n",
    "   - Missing data (NaNs) are imputed using cubic spline interpolation along the time axis. This helps create a fully dense tensor suitable for many ML methods.\n",
    "   \n",
    "   - If a feature for a given subject is too sparsely sampled (e.g., fewer than two non-NaN points), that feature is skipped for that subject.\n",
    "\n",
    "10. **Feature Filtering:**\n",
    "    - Additional filtering steps identify features that have sufficient non-NaN coverage across a chosen set of timepoints.\n",
    "    - Also, certain target features of interest are searched within the combined feature set to verify their presence and indices.\n",
    "\n",
    "11. **Saving Results:**\n",
    "    - The final imputed tensor and metadata (feature sets, temporal mappings, indices) are saved as pickle files for future use. This ensures that subsequent analyses can start from a well-structured, clean dataset.\n",
    "\n",
    "#### Key Outputs:\n",
    "- **`combined_dataframe`**: A single large table containing all subjects, features, and timepoints.\n",
    "- **`Data_tensor_on_cpu`**: The raw tensor before imputation.\n",
    "- **`imputed_tensor`**: The final tensor after cubic spline interpolation, ready for modeling.\n",
    "- **`imputed_tensor_train_data.pkl`**: A serialized pickle file containing the imputed tensor and related metadata.\n",
    "- **`training_data_Tensor_save.pkl`** and other pickle files: Intermediate and final data structures for debugging, backups, and downstream analyses.\n",
    "\n",
    "This modularizes the entire data processing... The resulting clean, tensor format sets the stage for machine learning ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "/tmp/ipykernel_1866916/673146385.py:95: RuntimeWarning: All-NaN slice encountered\n",
      "  baseline_medians = np.nanmedian(baseline_values, axis=1)\n",
      "/tmp/ipykernel_1866916/673146385.py:95: RuntimeWarning: All-NaN slice encountered\n",
      "  baseline_medians = np.nanmedian(baseline_values, axis=1)\n",
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "/tmp/ipykernel_1866916/673146385.py:95: RuntimeWarning: All-NaN slice encountered\n",
      "  baseline_medians = np.nanmedian(baseline_values, axis=1)\n",
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "Processing Groups: 100%|██████████| 94248/94248 [07:15<00:00, 216.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of features with non-NaN values at specified bounds per subject:\n",
      "Subject ID 0:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 1:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 2:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 3:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 4:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 5:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 6:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 7:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 8:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 9:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 10:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 11:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 12:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 13:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 14:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 15:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 16:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 17:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 18:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 19:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 20:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 21:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 22:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 23:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 24:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 25:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 26:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 27:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 28:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 29:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 30:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 31:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 32:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 33:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 34:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 35:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 36:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 37:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 38:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 39:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 40:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 41:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 42:\n",
      "  Bounds (0, 10): 81 features\n",
      "  Bounds (0, 16): 81 features\n",
      "  Bounds (0, 26): 81 features\n",
      "Subject ID 43:\n",
      "  Bounds (0, 10): 81 features\n",
      "  Bounds (0, 16): 81 features\n",
      "  Bounds (0, 26): 81 features\n",
      "Subject ID 44:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 45:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 46:\n",
      "  Bounds (0, 10): 51 features\n",
      "  Bounds (0, 16): 51 features\n",
      "  Bounds (0, 26): 51 features\n",
      "Subject ID 47:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 48:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 49:\n",
      "  Bounds (0, 10): 6684 features\n",
      "  Bounds (0, 16): 6684 features\n",
      "  Bounds (0, 26): 6684 features\n",
      "Subject ID 50:\n",
      "  Bounds (0, 10): 6634 features\n",
      "  Bounds (0, 16): 6634 features\n",
      "  Bounds (0, 26): 6634 features\n",
      "Subject ID 51:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 52:\n",
      "  Bounds (0, 10): 51 features\n",
      "  Bounds (0, 16): 51 features\n",
      "  Bounds (0, 26): 51 features\n",
      "Subject ID 53:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 54:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 55:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 56:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 57:\n",
      "  Bounds (0, 10): 31 features\n",
      "  Bounds (0, 16): 31 features\n",
      "  Bounds (0, 26): 31 features\n",
      "Subject ID 58:\n",
      "  Bounds (0, 10): 6692 features\n",
      "  Bounds (0, 16): 6692 features\n",
      "  Bounds (0, 26): 6692 features\n",
      "Subject ID 59:\n",
      "  Bounds (0, 10): 6695 features\n",
      "  Bounds (0, 16): 6695 features\n",
      "  Bounds (0, 26): 6695 features\n",
      "Subject ID 60:\n",
      "  Bounds (0, 10): 6725 features\n",
      "  Bounds (0, 16): 6725 features\n",
      "  Bounds (0, 26): 6725 features\n",
      "Subject ID 61:\n",
      "  Bounds (0, 10): 6725 features\n",
      "  Bounds (0, 16): 6725 features\n",
      "  Bounds (0, 26): 6725 features\n",
      "Subject ID 62:\n",
      "  Bounds (0, 10): 6725 features\n",
      "  Bounds (0, 16): 6725 features\n",
      "  Bounds (0, 26): 6725 features\n",
      "Subject ID 63:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 64:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 65:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 66:\n",
      "  Bounds (0, 10): 6728 features\n",
      "  Bounds (0, 16): 6728 features\n",
      "  Bounds (0, 26): 6728 features\n",
      "Subject ID 67:\n",
      "  Bounds (0, 10): 6726 features\n",
      "  Bounds (0, 16): 6726 features\n",
      "  Bounds (0, 26): 6726 features\n",
      "Subject ID 68:\n",
      "  Bounds (0, 10): 6730 features\n",
      "  Bounds (0, 16): 6730 features\n",
      "  Bounds (0, 26): 6730 features\n",
      "Subject ID 69:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 70:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 71:\n",
      "  Bounds (0, 10): 6733 features\n",
      "  Bounds (0, 16): 6733 features\n",
      "  Bounds (0, 26): 6733 features\n",
      "Subject ID 72:\n",
      "  Bounds (0, 10): 6686 features\n",
      "  Bounds (0, 16): 6686 features\n",
      "  Bounds (0, 26): 6686 features\n",
      "Subject ID 73:\n",
      "  Bounds (0, 10): 6724 features\n",
      "  Bounds (0, 16): 6724 features\n",
      "  Bounds (0, 26): 6724 features\n",
      "Subject ID 74:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 75:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 76:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 77:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 78:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 79:\n",
      "  Bounds (0, 10): 6700 features\n",
      "  Bounds (0, 16): 6700 features\n",
      "  Bounds (0, 26): 6700 features\n",
      "Subject ID 80:\n",
      "  Bounds (0, 10): 6725 features\n",
      "  Bounds (0, 16): 6725 features\n",
      "  Bounds (0, 26): 6725 features\n",
      "Subject ID 81:\n",
      "  Bounds (0, 10): 6728 features\n",
      "  Bounds (0, 16): 6728 features\n",
      "  Bounds (0, 26): 6728 features\n",
      "Subject ID 82:\n",
      "  Bounds (0, 10): 6725 features\n",
      "  Bounds (0, 16): 6725 features\n",
      "  Bounds (0, 26): 6725 features\n",
      "Subject ID 83:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 84:\n",
      "  Bounds (0, 10): 6687 features\n",
      "  Bounds (0, 16): 6687 features\n",
      "  Bounds (0, 26): 6687 features\n",
      "Subject ID 85:\n",
      "  Bounds (0, 10): 6687 features\n",
      "  Bounds (0, 16): 6687 features\n",
      "  Bounds (0, 26): 6687 features\n",
      "Subject ID 86:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 87:\n",
      "  Bounds (0, 10): 6731 features\n",
      "  Bounds (0, 16): 6731 features\n",
      "  Bounds (0, 26): 6731 features\n",
      "Subject ID 88:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 89:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 90:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 91:\n",
      "  Bounds (0, 10): 6733 features\n",
      "  Bounds (0, 16): 6733 features\n",
      "  Bounds (0, 26): 6733 features\n",
      "Subject ID 92:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 93:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 94:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 95:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 96:\n",
      "  Bounds (0, 10): 6725 features\n",
      "  Bounds (0, 16): 6725 features\n",
      "  Bounds (0, 26): 6725 features\n",
      "Subject ID 97:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 98:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 99:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 100:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 101:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 102:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 103:\n",
      "  Bounds (0, 10): 6731 features\n",
      "  Bounds (0, 16): 6731 features\n",
      "  Bounds (0, 26): 6731 features\n",
      "Subject ID 104:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 105:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 106:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 107:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 108:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 109:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 110:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 111:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Subject ID 112:\n",
      "  Bounds (0, 10): 6727 features\n",
      "  Bounds (0, 16): 6727 features\n",
      "  Bounds (0, 26): 6727 features\n",
      "Subject ID 113:\n",
      "  Bounds (0, 10): 6722 features\n",
      "  Bounds (0, 16): 6722 features\n",
      "  Bounds (0, 26): 6722 features\n",
      "Subject ID 114:\n",
      "  Bounds (0, 10): 6734 features\n",
      "  Bounds (0, 16): 6734 features\n",
      "  Bounds (0, 26): 6734 features\n",
      "Number of subjects: 22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_1_path = 'harmonized_data/train/TheData.xlsx'\n",
    "file_2_path = 'harmonized_data/train/training_pbmc_gene_expression_wide_tpm.tsv'\n",
    "file_path = 'harmonized_data/genenames.tsv'\n",
    "# Ensure the folder exists\n",
    "os.makedirs('./tmp/',  exist_ok=True)\n",
    "\n",
    "tpm_data = pd.read_csv(file_2_path, sep='\\t')\n",
    "genenames_df = pd.read_csv(file_path, sep='\\t')\n",
    "genenames_array = genenames_df.iloc[:, 0].to_numpy()\n",
    "columns_to_keep = ['specimen_id'] + list(genenames_array)\n",
    "tpm_data = tpm_data.loc[:, tpm_data.columns.intersection(columns_to_keep)]\n",
    "excel_data = pd.ExcelFile(file_1_path)\n",
    "sheet_dataframes = {sheet: excel_data.parse(sheet) for sheet in excel_data.sheet_names}\n",
    "sheet_columns = {sheet: df.columns.tolist() for sheet, df in sheet_dataframes.items()}\n",
    "\n",
    "subject_specimen_df = sheet_dataframes['subject_specimen']\n",
    "if 'actual_day_relative_to_boost' in subject_specimen_df.columns:\n",
    "    specimen_time_mapping = subject_specimen_df[['specimen_id', 'actual_day_relative_to_boost']]\n",
    "    specimen_time_mapping = specimen_time_mapping.rename(columns={'actual_day_relative_to_boost': 'timepoint'})\n",
    "else:\n",
    "    raise ValueError(\"Timepoint data is missing from the 'subject_specimen' sheet.\")\n",
    "\n",
    "timeseries_dataframes = {}\n",
    "for sheet_name, df in sheet_dataframes.items():\n",
    "    if sheet_name != 'subject_specimen':\n",
    "        df_with_time = pd.merge(df, specimen_time_mapping, on='specimen_id', how='left')\n",
    "        time_series = df_with_time.pivot(index='specimen_id', columns='timepoint')\n",
    "        timeseries_dataframes[sheet_name] = time_series\n",
    "\n",
    "timeseries_shapes = {sheet: df.shape for sheet, df in timeseries_dataframes.items()}\n",
    "\n",
    "if 'actual_day_relative_to_boost' in subject_specimen_df.columns:\n",
    "    subject_specimen_mapping = subject_specimen_df[['specimen_id', 'subject_id', 'actual_day_relative_to_boost']]\n",
    "else:\n",
    "    subject_specimen_mapping = subject_specimen_df[['specimen_id', 'subject_id', 'planned_day_relative_to_boost']]\n",
    "    subject_specimen_mapping = subject_specimen_mapping.rename(columns={'planned_day_relative_to_boost': 'actual_day_relative_to_boost'})\n",
    "\n",
    "restructured_dataframes_corrected = {}\n",
    "for sheet_name, df in timeseries_dataframes.items():\n",
    "    df_reset = df.reset_index()\n",
    "    df_reset.columns = [\n",
    "        '_'.join(map(str, col)) if isinstance(col, tuple) else col for col in df_reset.columns\n",
    "    ]\n",
    "    if 'specimen_id_' in df_reset.columns:\n",
    "        df_reset = df_reset.rename(columns={'specimen_id_': 'specimen_id'})\n",
    "    df_with_mapping = pd.merge(df_reset, subject_specimen_mapping, on='specimen_id', how='left')\n",
    "    df_pivoted = df_with_mapping.pivot_table(\n",
    "        index='subject_id',\n",
    "        columns=['actual_day_relative_to_boost'],\n",
    "        values=[col for col in df_with_mapping.columns if col not in ['specimen_id', 'subject_id', 'actual_day_relative_to_boost']]\n",
    "    )\n",
    "    df_pivoted.columns = [\n",
    "        f\"{col[0]}_day{col[1]}\" if isinstance(col, tuple) else col for col in df_pivoted.columns\n",
    "    ]\n",
    "    restructured_dataframes_corrected[sheet_name] = df_pivoted\n",
    "\n",
    "t_cell_polarization_df = restructured_dataframes_corrected['t_cell_polarization']\n",
    "t_cell_activation_df = restructured_dataframes_corrected['t_cell_activation']\n",
    "plasma_cytokine_conc_O_df = restructured_dataframes_corrected['plasma_cytokine_conc_O']\n",
    "plasma_cytokine_conc_L_df = restructured_dataframes_corrected['plasma_cytokine_conc_L']\n",
    "plasma_antibody_levels_df = restructured_dataframes_corrected['plasma_antibody_levels']\n",
    "pbmc_cell_frequency_df = restructured_dataframes_corrected['pbmc_cell_frequency']\n",
    "\n",
    "available_baseline_timepoints = subject_specimen_mapping['actual_day_relative_to_boost'].unique()\n",
    "baseline_timepoints_scanned = sorted([tp for tp in available_baseline_timepoints if tp in available_baseline_timepoints[available_baseline_timepoints<0]])\n",
    "baseline_timepoints = baseline_timepoints_scanned\n",
    "\n",
    "tpm_data_mapped = pd.merge(\n",
    "    tpm_data,\n",
    "    subject_specimen_mapping[['specimen_id', 'subject_id', 'actual_day_relative_to_boost']],\n",
    "    on='specimen_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "tpm_data_pivoted = tpm_data_mapped.pivot_table(\n",
    "    index='subject_id',\n",
    "    columns='actual_day_relative_to_boost',\n",
    "    values=[col for col in tpm_data_mapped.columns if col not in ['specimen_id', 'subject_id', 'actual_day_relative_to_boost']]\n",
    ")\n",
    "tpm_data_pivoted.columns = [f\"{col[0]}_day{col[1]}\" for col in tpm_data_pivoted.columns]\n",
    "tpm_data_final = tpm_data_pivoted\n",
    "\n",
    "np.seterr(invalid=\"ignore\")\n",
    "\n",
    "def consolidate_baseline_features_handle_nan(df, baseline_timepoints):\n",
    "    baseline_columns = [col for col in df.columns if any(f\"day{tp}\" in col for tp in baseline_timepoints)]\n",
    "    non_baseline_columns = [col for col in df.columns if col not in baseline_columns]\n",
    "    if not baseline_columns:\n",
    "        return df\n",
    "    baseline_values = df[baseline_columns].to_numpy()\n",
    "    baseline_medians = np.nanmedian(baseline_values, axis=1)\n",
    "    all_nan_rows = np.isnan(baseline_values).all(axis=1)\n",
    "    baseline_medians[all_nan_rows] = np.nan\n",
    "    baseline_feature_names = set(col.split(\"_day\")[0] for col in baseline_columns)\n",
    "    consolidated_baseline = pd.DataFrame(\n",
    "        {f\"{feature}_day0.0\": baseline_medians for feature in baseline_feature_names},\n",
    "        index=df.index\n",
    "    )\n",
    "    non_baseline_df = df[non_baseline_columns]\n",
    "    consolidated_df = pd.concat([consolidated_baseline, non_baseline_df], axis=1)\n",
    "    return consolidated_df\n",
    "\n",
    "def process_large_dataframe_in_chunks(df, baseline_timepoints, chunk_size=50000):\n",
    "    column_names = df.columns\n",
    "    num_chunks = (len(column_names) // chunk_size) + 1\n",
    "    result_dfs = []\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min(start_idx + chunk_size, len(column_names))\n",
    "        chunk_columns = column_names[start_idx:end_idx]\n",
    "        chunk_df = df[chunk_columns]\n",
    "        processed_chunk = consolidate_baseline_features_handle_nan(chunk_df, baseline_timepoints)\n",
    "        result_dfs.append(processed_chunk)\n",
    "    return pd.concat(result_dfs, axis=1)\n",
    "\n",
    "dataframes = {\n",
    "    \"tpm_data_final\": tpm_data_final,\n",
    "    \"t_cell_polarization_df\": t_cell_polarization_df,\n",
    "    \"t_cell_activation_df\": t_cell_activation_df,\n",
    "    \"plasma_cytokine_conc_O_df\": plasma_cytokine_conc_O_df,\n",
    "    \"plasma_cytokine_conc_L_df\": plasma_cytokine_conc_L_df,\n",
    "    \"plasma_antibody_levels_df\": plasma_antibody_levels_df,\n",
    "    \"pbmc_cell_frequency_df\": pbmc_cell_frequency_df\n",
    "}\n",
    "\n",
    "processed_dataframes = {}\n",
    "processing_status = {}\n",
    "for name, df in dataframes.items():\n",
    "    try:\n",
    "        if df.shape[1] > 100000:\n",
    "            processed_dataframes[name] = process_large_dataframe_in_chunks(df, baseline_timepoints, chunk_size=50000)\n",
    "        else:\n",
    "            processed_dataframes[name] = consolidate_baseline_features_handle_nan(df, baseline_timepoints)\n",
    "        processing_status[name] = \"completed\"\n",
    "    except Exception as e:\n",
    "        processing_status[name] = f\"error: {e}\"\n",
    "\n",
    "tpm_data_final_processed = processed_dataframes.get(\"tpm_data_final\", None)\n",
    "t_cell_polarization_df_processed = processed_dataframes.get(\"t_cell_polarization_df\", None)\n",
    "t_cell_activation_df_processed = processed_dataframes.get(\"t_cell_activation_df\", None)\n",
    "plasma_cytokine_conc_O_df_processed = processed_dataframes.get(\"plasma_cytokine_conc_O_df\", None)\n",
    "plasma_cytokine_conc_L_df_processed = processed_dataframes.get(\"plasma_cytokine_conc_L_df\", None)\n",
    "plasma_antibody_levels_df_processed = processed_dataframes.get(\"plasma_antibody_levels_df\", None)\n",
    "pbmc_cell_frequency_df_processed = processed_dataframes.get(\"pbmc_cell_frequency_df\", None)\n",
    "\n",
    "dataframes_to_combine = [\n",
    "    pbmc_cell_frequency_df_processed,\n",
    "    plasma_antibody_levels_df_processed,\n",
    "    plasma_cytokine_conc_L_df_processed,\n",
    "    plasma_cytokine_conc_O_df_processed,\n",
    "    t_cell_activation_df_processed,\n",
    "    t_cell_polarization_df_processed,\n",
    "    tpm_data_final_processed\n",
    "]\n",
    "\n",
    "combined_dataframe = None\n",
    "for df in dataframes_to_combine:\n",
    "    if df is not None:\n",
    "        if combined_dataframe is None:\n",
    "            combined_dataframe = df\n",
    "        else:\n",
    "            combined_dataframe = pd.merge(combined_dataframe, df, on=\"subject_id\", how=\"outer\")\n",
    "\n",
    "combined_dataframe_shape = combined_dataframe.shape if combined_dataframe is not None else (0, 0)\n",
    "\n",
    "def parse_features_and_temporal_dimensions_refined(df):\n",
    "    temporal_columns = [col for col in df.columns if \"_day\" in col]\n",
    "    parsed_data = []\n",
    "    for col in temporal_columns:\n",
    "        try:\n",
    "            feature_name, temporal_info = col.rsplit(\"_day\", 1)\n",
    "            temporal_parts = temporal_info.split(\"_\")\n",
    "            temporal_dimension = int(float(temporal_parts[0]))\n",
    "            if len(temporal_parts) > 1:\n",
    "                suffix = \"_\" + temporal_parts[-1]\n",
    "                feature_name += suffix\n",
    "            feature_name_parts = feature_name.split(\"_\")\n",
    "            feature_name = \"_\".join(\n",
    "                part for part in feature_name_parts if not part.replace(\".\", \"\").replace(\"-\", \"\").isdigit()\n",
    "            )\n",
    "            parsed_data.append({\n",
    "                \"feature_name\": feature_name,\n",
    "                \"temporal_dimension\": temporal_dimension,\n",
    "                \"column_name\": col\n",
    "            })\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.DataFrame(parsed_data)\n",
    "\n",
    "parsed_features_complex_df = parse_features_and_temporal_dimensions_refined(combined_dataframe)\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.interpolate import CubicSpline\n",
    "from tqdm import tqdm\n",
    "\n",
    "unique_temporal_values = np.sort(parsed_features_complex_df[\"temporal_dimension\"].unique())\n",
    "temporal_mapping = {f\"T{i}\": (i, value) for i, value in enumerate(unique_temporal_values)}\n",
    "unique_features = np.sort(parsed_features_complex_df[\"feature_name\"].unique())\n",
    "target_features = [\n",
    "    \"ENSG00000277632.1\",\n",
    "    \"DMSO_P01579\", \"DMSO_Q16552\", \"DMSO_P05113\", \"PHA_P01579\",\n",
    "    \"PHA_Q16552\", \"PHA_P05113\", \"PT_P01579\", \"PT_Q16552\", \"PT_P05113\",\n",
    "    \"Monocytes\", \"IgG_PT\"\n",
    "]\n",
    "\n",
    "FeaturesToIncludeIndices = [\n",
    "    np.where(unique_features == feature)[0][0] if feature in unique_features else -1\n",
    "    for feature in target_features\n",
    "]\n",
    "\n",
    "def create_3d_tensor_with_assertions(df, parsed_features_df, device=\"cpu\"):\n",
    "    unique_subjects = df.index.astype(int).unique()\n",
    "    unique_temporal_dimensions = np.sort(parsed_features_df[\"temporal_dimension\"].unique())\n",
    "    unique_features_local = np.sort(parsed_features_df[\"feature_name\"].unique())\n",
    "    N, T, F = len(unique_subjects), len(unique_temporal_dimensions), len(unique_features_local)\n",
    "    tensor = torch.full((N, T, F), float('nan'), dtype=torch.float, device=device)\n",
    "    subject_to_idx = {subject: i for i, subject in enumerate(unique_subjects)}\n",
    "    temporal_to_idx = {t: i for i, t in enumerate(unique_temporal_dimensions)}\n",
    "    feature_to_idx = {feature: i for i, feature in enumerate(unique_features_local)}\n",
    "    parsed_features_df[\"feature_idx\"] = parsed_features_df[\"feature_name\"].map(feature_to_idx)\n",
    "    parsed_features_df[\"temporal_idx\"] = parsed_features_df[\"temporal_dimension\"].map(temporal_to_idx)\n",
    "    valid_parsed_features = parsed_features_df[parsed_features_df[\"column_name\"].isin(df.columns)]\n",
    "    grouped = valid_parsed_features.groupby([\"feature_idx\", \"temporal_idx\"])\n",
    "    for (feature_idx, temporal_idx), group in tqdm(grouped, desc=\"Processing Groups\", total=len(grouped)):\n",
    "        columns = group[\"column_name\"].values\n",
    "        valid_columns = [col for col in columns if col in df.columns]\n",
    "        if not valid_columns:\n",
    "            continue\n",
    "        aggregated_values = df[valid_columns].min(axis=1, skipna=True).values\n",
    "        subject_indices = torch.tensor(df.index.map(subject_to_idx).values, dtype=torch.long, device=device)\n",
    "        tensor[subject_indices, temporal_idx, feature_idx] = torch.tensor(aggregated_values, dtype=torch.float, device=device)\n",
    "    return tensor, unique_subjects, unique_temporal_dimensions, unique_features_local\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tensor_3d_with_assertions, subjects, temporal_dims, features = create_3d_tensor_with_assertions(\n",
    "    combined_dataframe, parsed_features_complex_df, device=device\n",
    ")\n",
    "Data_tensor_on_cpu = tensor_3d_with_assertions.cpu()\n",
    "\n",
    "with open(\"./tmp/training_data_Tensor_save.pkl\", \"wb\") as f:\n",
    "    pickle.dump(Data_tensor_on_cpu, f)\n",
    "\n",
    "def impute_with_cubic_interpolation_safe(tensor, time_dim=1):\n",
    "    device = tensor.device\n",
    "    tensor = tensor.cpu()\n",
    "    imputed_tensor = tensor.clone()\n",
    "    skipped_features = []\n",
    "    for i in range(tensor.size(0)):\n",
    "        for j in range(tensor.size(2)):\n",
    "            data = tensor[i, :, j].numpy()\n",
    "            time_indices = ~torch.isnan(tensor[i, :, j])\n",
    "            times = torch.arange(tensor.size(1))\n",
    "            if time_indices.sum() < 2:\n",
    "                skipped_features.append(j)\n",
    "                continue\n",
    "            cubic_spline = CubicSpline(times[time_indices], data[time_indices], extrapolate=True)\n",
    "            imputed_values = cubic_spline(times)\n",
    "            imputed_tensor[i, :, j] = torch.tensor(imputed_values, dtype=torch.float32)\n",
    "    skipped_features = list(set(skipped_features))\n",
    "    return imputed_tensor.to(device), skipped_features\n",
    "\n",
    "imputed_tensor, skipped_features = impute_with_cubic_interpolation_safe(Data_tensor_on_cpu, time_dim=1)\n",
    "\n",
    "def get_non_nan_features_for_timepoints(tensor, timepoints):\n",
    "    valid_features_set = set(range(tensor.size(2)))\n",
    "    batch_size = tensor.size(0)\n",
    "    threshold = batch_size/2\n",
    "    for timepoint in timepoints:\n",
    "        non_nan_counts = (~torch.isnan(tensor[:, timepoint, :])).sum(dim=0)\n",
    "        valid_features_mask = non_nan_counts >= threshold\n",
    "        valid_feature_indices = set(torch.nonzero(valid_features_mask, as_tuple=True)[0].tolist())\n",
    "        valid_features_set &= valid_feature_indices\n",
    "    return torch.tensor(sorted(valid_features_set))\n",
    "\n",
    "timepoints_to_check = [0, 10, 16]\n",
    "non_nan_features_indices = get_non_nan_features_for_timepoints(imputed_tensor, timepoints_to_check)\n",
    "\n",
    "if isinstance(non_nan_features_indices, torch.Tensor):\n",
    "    non_nan_features_indices_list = non_nan_features_indices.tolist()\n",
    "\n",
    "combined_features = sorted(set(non_nan_features_indices_list + FeaturesToIncludeIndices))\n",
    "unique_features_combined = unique_features[combined_features]\n",
    "\n",
    "TargetedFeaturesToEvaluate_combined = [\n",
    "    np.where(unique_features_combined == feature)[0][0] if feature in unique_features_combined else -1\n",
    "    for feature in target_features\n",
    "]\n",
    "\n",
    "mapped_feature_pairs = {\n",
    "    feature: (TargetedFeaturesToEvaluate_combined[i] if TargetedFeaturesToEvaluate_combined[i] != -1 else \"Not Found\")\n",
    "    for i, feature in enumerate(target_features)\n",
    "}\n",
    "\n",
    "\n",
    "def count_features_with_non_nan_bounds_per_subject(tensor, bounds):\n",
    "    counts_per_subject = {}\n",
    "    for subject_id in range(tensor.size(0)):\n",
    "        counts = {\n",
    "            (start, end): (~torch.isnan(tensor[subject_id, start, :]) & ~torch.isnan(tensor[subject_id, end, :])).sum().item()\n",
    "            for start, end in bounds\n",
    "        }\n",
    "        counts_per_subject[subject_id] = counts\n",
    "    return counts_per_subject\n",
    "\n",
    "bounds = [(0, 10), (0, 16), (0, 26)]\n",
    "feature_counts_per_subject = count_features_with_non_nan_bounds_per_subject(imputed_tensor, bounds)\n",
    "\n",
    "# print(\"Counts of features with non-NaN values at specified bounds per subject:\")\n",
    "# for subject_id, counts in feature_counts_per_subject.items():\n",
    "#     print(f\"Subject ID {subject_id}:\")\n",
    "#     for bound, count in counts.items():\n",
    "#         print(f\"  Bounds {bound}: {count} features\")\n",
    "\n",
    "min_threshold = 6000\n",
    "subject_ids_with_low_features_count = [\n",
    "    subject_id\n",
    "    for subject_id, counts in feature_counts_per_subject.items()\n",
    "    if all(count < min_threshold for count in counts.values())\n",
    "]\n",
    "\n",
    "# print(f\"\\nSubject IDs with all bounds having less than {min_threshold} features:\")\n",
    "# print(subject_ids_with_low_features_count)\n",
    "print(f\"Number of subjects: {len(subject_ids_with_low_features_count)}\")\n",
    "\n",
    "\n",
    "variables_to_save = {\n",
    "    \"imputed_tensor\": imputed_tensor,\n",
    "    \"combined_features\": combined_features,\n",
    "    \"unique_features\": unique_features,\n",
    "    \"unique_features_combined\": unique_features_combined,\n",
    "    \"subject_ids_with_low_features_count\": subject_ids_with_low_features_count,\n",
    "    \"TargetedFeaturesToEvaluate_combined\": TargetedFeaturesToEvaluate_combined,\n",
    "    \"unique_temporal_values\": unique_temporal_values,\n",
    "    \"temporal_mapping\": temporal_mapping\n",
    "}\n",
    "\n",
    "with open(\"./tmp/imputed_tensor_train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(variables_to_save, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "Processing Groups: 100%|██████████| 6756/6756 [00:13<00:00, 502.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final tensor shape: torch.Size([54, 1, 6693])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_1_path = 'harmonized_data/challenge/TheData_challenge.xlsx'\n",
    "file_2_path = 'harmonized_data/challenge/challenge_pbmc_gene_expression_wide_tpm.tsv'\n",
    "file_path = 'harmonized_data/genenames.tsv'\n",
    "# Ensure the folder exists\n",
    "os.makedirs('./tmp/',  exist_ok=True)\n",
    "\n",
    "tpm_data = pd.read_csv(file_2_path, sep='\\t')\n",
    "genenames_df = pd.read_csv(file_path, sep='\\t')\n",
    "genenames_array = genenames_df.iloc[:, 0].to_numpy()\n",
    "columns_to_keep = ['specimen_id'] + list(genenames_array)\n",
    "tpm_data = tpm_data.loc[:, tpm_data.columns.intersection(columns_to_keep)]\n",
    "excel_data = pd.ExcelFile(file_1_path)\n",
    "sheet_dataframes = {sheet: excel_data.parse(sheet) for sheet in excel_data.sheet_names}\n",
    "sheet_columns = {sheet: df.columns.tolist() for sheet, df in sheet_dataframes.items()}\n",
    "\n",
    "subject_specimen_df = sheet_dataframes['subject_specimen']\n",
    "if 'actual_day_relative_to_boost' in subject_specimen_df.columns:\n",
    "    specimen_time_mapping = subject_specimen_df[['specimen_id', 'actual_day_relative_to_boost']]\n",
    "    specimen_time_mapping = specimen_time_mapping.rename(columns={'actual_day_relative_to_boost': 'timepoint'})\n",
    "else:\n",
    "    raise ValueError(\"Timepoint data is missing from the 'subject_specimen' sheet.\")\n",
    "\n",
    "timeseries_dataframes = {}\n",
    "for sheet_name, df in sheet_dataframes.items():\n",
    "    if sheet_name != 'subject_specimen':\n",
    "        df_with_time = pd.merge(df, specimen_time_mapping, on='specimen_id', how='left')\n",
    "        time_series = df_with_time.pivot(index='specimen_id', columns='timepoint')\n",
    "        timeseries_dataframes[sheet_name] = time_series\n",
    "\n",
    "timeseries_shapes = {sheet: df.shape for sheet, df in timeseries_dataframes.items()}\n",
    "\n",
    "if 'actual_day_relative_to_boost' in subject_specimen_df.columns:\n",
    "    subject_specimen_mapping = subject_specimen_df[['specimen_id', 'subject_id', 'actual_day_relative_to_boost']]\n",
    "else:\n",
    "    subject_specimen_mapping = subject_specimen_df[['specimen_id', 'subject_id', 'planned_day_relative_to_boost']]\n",
    "    subject_specimen_mapping = subject_specimen_mapping.rename(columns={'planned_day_relative_to_boost': 'actual_day_relative_to_boost'})\n",
    "\n",
    "restructured_dataframes_corrected = {}\n",
    "for sheet_name, df in timeseries_dataframes.items():\n",
    "    df_reset = df.reset_index()\n",
    "    df_reset.columns = [\n",
    "        '_'.join(map(str, col)) if isinstance(col, tuple) else col for col in df_reset.columns\n",
    "    ]\n",
    "    if 'specimen_id_' in df_reset.columns:\n",
    "        df_reset = df_reset.rename(columns={'specimen_id_': 'specimen_id'})\n",
    "    df_with_mapping = pd.merge(df_reset, subject_specimen_mapping, on='specimen_id', how='left')\n",
    "    df_pivoted = df_with_mapping.pivot_table(\n",
    "        index='subject_id',\n",
    "        columns=['actual_day_relative_to_boost'],\n",
    "        values=[col for col in df_with_mapping.columns if col not in ['specimen_id', 'subject_id', 'actual_day_relative_to_boost']]\n",
    "    )\n",
    "    df_pivoted.columns = [\n",
    "        f\"{col[0]}_day{col[1]}\" if isinstance(col, tuple) else col for col in df_pivoted.columns\n",
    "    ]\n",
    "    restructured_dataframes_corrected[sheet_name] = df_pivoted\n",
    "\n",
    "t_cell_polarization_df = restructured_dataframes_corrected['t_cell_polarization']\n",
    "t_cell_activation_df = restructured_dataframes_corrected['t_cell_activation']\n",
    "plasma_cytokine_conc_O_df = restructured_dataframes_corrected['plasma_cytokine_conc_O']\n",
    "plasma_cytokine_conc_L_df = restructured_dataframes_corrected['plasma_cytokine_conc_L']\n",
    "plasma_antibody_levels_df = restructured_dataframes_corrected['plasma_antibody_levels']\n",
    "pbmc_cell_frequency_df = restructured_dataframes_corrected['pbmc_cell_frequency']\n",
    "\n",
    "available_baseline_timepoints = subject_specimen_mapping['actual_day_relative_to_boost'].unique()\n",
    "baseline_timepoints_scanned = sorted([tp for tp in available_baseline_timepoints if tp in available_baseline_timepoints[available_baseline_timepoints<0]])\n",
    "baseline_timepoints = baseline_timepoints_scanned\n",
    "\n",
    "tpm_data_mapped = pd.merge(\n",
    "    tpm_data,\n",
    "    subject_specimen_mapping[['specimen_id', 'subject_id', 'actual_day_relative_to_boost']],\n",
    "    on='specimen_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "tpm_data_pivoted = tpm_data_mapped.pivot_table(\n",
    "    index='subject_id',\n",
    "    columns='actual_day_relative_to_boost',\n",
    "    values=[col for col in tpm_data_mapped.columns if col not in ['specimen_id', 'subject_id', 'actual_day_relative_to_boost']]\n",
    ")\n",
    "tpm_data_pivoted.columns = [f\"{col[0]}_day{col[1]}\" for col in tpm_data_pivoted.columns]\n",
    "tpm_data_final = tpm_data_pivoted\n",
    "\n",
    "np.seterr(invalid=\"ignore\")\n",
    "\n",
    "def consolidate_baseline_features_handle_nan(df, baseline_timepoints):\n",
    "    baseline_columns = [col for col in df.columns if any(f\"day{tp}\" in col for tp in baseline_timepoints)]\n",
    "    non_baseline_columns = [col for col in df.columns if col not in baseline_columns]\n",
    "    if not baseline_columns:\n",
    "        return df\n",
    "    baseline_values = df[baseline_columns].to_numpy()\n",
    "    baseline_medians = np.nanmedian(baseline_values, axis=1)\n",
    "    all_nan_rows = np.isnan(baseline_values).all(axis=1)\n",
    "    baseline_medians[all_nan_rows] = np.nan\n",
    "    baseline_feature_names = set(col.split(\"_day\")[0] for col in baseline_columns)\n",
    "    consolidated_baseline = pd.DataFrame(\n",
    "        {f\"{feature}_day0.0\": baseline_medians for feature in baseline_feature_names},\n",
    "        index=df.index\n",
    "    )\n",
    "    non_baseline_df = df[non_baseline_columns]\n",
    "    consolidated_df = pd.concat([consolidated_baseline, non_baseline_df], axis=1)\n",
    "    return consolidated_df\n",
    "\n",
    "def process_large_dataframe_in_chunks(df, baseline_timepoints, chunk_size=50000):\n",
    "    column_names = df.columns\n",
    "    num_chunks = (len(column_names) // chunk_size) + 1\n",
    "    result_dfs = []\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min(start_idx + chunk_size, len(column_names))\n",
    "        chunk_columns = column_names[start_idx:end_idx]\n",
    "        chunk_df = df[chunk_columns]\n",
    "        processed_chunk = consolidate_baseline_features_handle_nan(chunk_df, baseline_timepoints)\n",
    "        result_dfs.append(processed_chunk)\n",
    "    return pd.concat(result_dfs, axis=1)\n",
    "\n",
    "dataframes = {\n",
    "    \"tpm_data_final\": tpm_data_final,\n",
    "    \"t_cell_polarization_df\": t_cell_polarization_df,\n",
    "    \"t_cell_activation_df\": t_cell_activation_df,\n",
    "    \"plasma_cytokine_conc_O_df\": plasma_cytokine_conc_O_df,\n",
    "    \"plasma_cytokine_conc_L_df\": plasma_cytokine_conc_L_df,\n",
    "    \"plasma_antibody_levels_df\": plasma_antibody_levels_df,\n",
    "    \"pbmc_cell_frequency_df\": pbmc_cell_frequency_df\n",
    "}\n",
    "\n",
    "processed_dataframes = {}\n",
    "processing_status = {}\n",
    "for name, df in dataframes.items():\n",
    "    try:\n",
    "        if df.shape[1] > 100000:\n",
    "            processed_dataframes[name] = process_large_dataframe_in_chunks(df, baseline_timepoints, chunk_size=50000)\n",
    "        else:\n",
    "            processed_dataframes[name] = consolidate_baseline_features_handle_nan(df, baseline_timepoints)\n",
    "        processing_status[name] = \"completed\"\n",
    "    except Exception as e:\n",
    "        processing_status[name] = f\"error: {e}\"\n",
    "\n",
    "tpm_data_final_processed = processed_dataframes.get(\"tpm_data_final\", None)\n",
    "t_cell_polarization_df_processed = processed_dataframes.get(\"t_cell_polarization_df\", None)\n",
    "t_cell_activation_df_processed = processed_dataframes.get(\"t_cell_activation_df\", None)\n",
    "plasma_cytokine_conc_O_df_processed = processed_dataframes.get(\"plasma_cytokine_conc_O_df\", None)\n",
    "plasma_cytokine_conc_L_df_processed = processed_dataframes.get(\"plasma_cytokine_conc_L_df\", None)\n",
    "plasma_antibody_levels_df_processed = processed_dataframes.get(\"plasma_antibody_levels_df\", None)\n",
    "pbmc_cell_frequency_df_processed = processed_dataframes.get(\"pbmc_cell_frequency_df\", None)\n",
    "\n",
    "dataframes_to_combine = [\n",
    "    pbmc_cell_frequency_df_processed,\n",
    "    plasma_antibody_levels_df_processed,\n",
    "    plasma_cytokine_conc_L_df_processed,\n",
    "    plasma_cytokine_conc_O_df_processed,\n",
    "    t_cell_activation_df_processed,\n",
    "    t_cell_polarization_df_processed,\n",
    "    tpm_data_final_processed\n",
    "]\n",
    "\n",
    "combined_dataframe = None\n",
    "for df in dataframes_to_combine:\n",
    "    if df is not None:\n",
    "        if combined_dataframe is None:\n",
    "            combined_dataframe = df\n",
    "        else:\n",
    "            combined_dataframe = pd.merge(combined_dataframe, df, on=\"subject_id\", how=\"outer\")\n",
    "\n",
    "combined_dataframe_shape = combined_dataframe.shape if combined_dataframe is not None else (0, 0)\n",
    "\n",
    "def parse_features_and_temporal_dimensions_refined(df):\n",
    "    temporal_columns = [col for col in df.columns if \"_day\" in col]\n",
    "    parsed_data = []\n",
    "    for col in temporal_columns:\n",
    "        try:\n",
    "            feature_name, temporal_info = col.rsplit(\"_day\", 1)\n",
    "            temporal_parts = temporal_info.split(\"_\")\n",
    "            temporal_dimension = int(float(temporal_parts[0]))\n",
    "            if len(temporal_parts) > 1:\n",
    "                suffix = \"_\" + temporal_parts[-1]\n",
    "                feature_name += suffix\n",
    "            feature_name_parts = feature_name.split(\"_\")\n",
    "            feature_name = \"_\".join(\n",
    "                part for part in feature_name_parts if not part.replace(\".\", \"\").replace(\"-\", \"\").isdigit()\n",
    "            )\n",
    "            parsed_data.append({\n",
    "                \"feature_name\": feature_name,\n",
    "                \"temporal_dimension\": temporal_dimension,\n",
    "                \"column_name\": col\n",
    "            })\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.DataFrame(parsed_data)\n",
    "\n",
    "parsed_features_complex_df = parse_features_and_temporal_dimensions_refined(combined_dataframe)\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.interpolate import CubicSpline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# unique_temporal_values = np.sort(parsed_features_complex_df[\"temporal_dimension\"].unique())\n",
    "# temporal_mapping = {f\"T{i}\": (i, value) for i, value in enumerate(unique_temporal_values)}\n",
    "# unique_features = np.sort(parsed_features_complex_df[\"feature_name\"].unique())\n",
    "# target_features = [\n",
    "#     \"ENSG00000277632.1\",\n",
    "#     \"DMSO_P01579\", \"DMSO_Q16552\", \"DMSO_P05113\", \"PHA_P01579\",\n",
    "#     \"PHA_Q16552\", \"PHA_P05113\", \"PT_P01579\", \"PT_Q16552\", \"PT_P05113\",\n",
    "#     \"Monocytes\", \"IgG_PT\"\n",
    "# ]\n",
    "\n",
    "# FeaturesToIncludeIndices = [\n",
    "#     np.where(unique_features == feature)[0][0] if feature in unique_features else -1\n",
    "#     for feature in target_features\n",
    "# ]\n",
    "\n",
    "def create_3d_tensor_with_assertions(df, parsed_features_df, device=\"cpu\"):\n",
    "    unique_subjects = df.index.astype(int).unique()\n",
    "    unique_temporal_dimensions = np.sort(parsed_features_df[\"temporal_dimension\"].unique())\n",
    "    unique_features_local = np.sort(parsed_features_df[\"feature_name\"].unique())\n",
    "    N, T, F = len(unique_subjects), len(unique_temporal_dimensions), len(unique_features_local)\n",
    "    tensor = torch.full((N, T, F), float('nan'), dtype=torch.float, device=device)\n",
    "    subject_to_idx = {subject: i for i, subject in enumerate(unique_subjects)}\n",
    "    temporal_to_idx = {t: i for i, t in enumerate(unique_temporal_dimensions)}\n",
    "    feature_to_idx = {feature: i for i, feature in enumerate(unique_features_local)}\n",
    "    parsed_features_df[\"feature_idx\"] = parsed_features_df[\"feature_name\"].map(feature_to_idx)\n",
    "    parsed_features_df[\"temporal_idx\"] = parsed_features_df[\"temporal_dimension\"].map(temporal_to_idx)\n",
    "    valid_parsed_features = parsed_features_df[parsed_features_df[\"column_name\"].isin(df.columns)]\n",
    "    grouped = valid_parsed_features.groupby([\"feature_idx\", \"temporal_idx\"])\n",
    "    for (feature_idx, temporal_idx), group in tqdm(grouped, desc=\"Processing Groups\", total=len(grouped)):\n",
    "        columns = group[\"column_name\"].values\n",
    "        valid_columns = [col for col in columns if col in df.columns]\n",
    "        if not valid_columns:\n",
    "            continue\n",
    "        aggregated_values = df[valid_columns].min(axis=1, skipna=True).values\n",
    "        subject_indices = torch.tensor(df.index.map(subject_to_idx).values, dtype=torch.long, device=device)\n",
    "        tensor[subject_indices, temporal_idx, feature_idx] = torch.tensor(aggregated_values, dtype=torch.float, device=device)\n",
    "    return tensor, unique_subjects, unique_temporal_dimensions, unique_features_local\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# tensor_3d_with_assertions, subjects, temporal_dims, features = create_3d_tensor_with_assertions(\n",
    "#     combined_dataframe, parsed_features_complex_df, device=device\n",
    "# )\n",
    "# Data_tensor_on_cpu = tensor_3d_with_assertions.cpu()\n",
    "\n",
    "# with open(\"./tmp/challenge_data_Tensor_save.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(Data_tensor_on_cpu, f)\n",
    "\n",
    "# def impute_with_cubic_interpolation_safe(tensor, time_dim=1):\n",
    "#     device = tensor.device\n",
    "#     tensor = tensor.cpu()\n",
    "#     imputed_tensor = tensor.clone()\n",
    "#     skipped_features = []\n",
    "#     for i in range(tensor.size(0)):\n",
    "#         for j in range(tensor.size(2)):\n",
    "#             data = tensor[i, :, j].numpy()\n",
    "#             time_indices = ~torch.isnan(tensor[i, :, j])\n",
    "#             times = torch.arange(tensor.size(1))\n",
    "#             if time_indices.sum() < 2:\n",
    "#                 skipped_features.append(j)\n",
    "#                 continue\n",
    "#             cubic_spline = CubicSpline(times[time_indices], data[time_indices], extrapolate=True)\n",
    "#             imputed_values = cubic_spline(times)\n",
    "#             imputed_tensor[i, :, j] = torch.tensor(imputed_values, dtype=torch.float32)\n",
    "#     skipped_features = list(set(skipped_features))\n",
    "#     return imputed_tensor.to(device), skipped_features\n",
    "\n",
    "# imputed_tensor, skipped_features = impute_with_cubic_interpolation_safe(Data_tensor_on_cpu, time_dim=1)\n",
    "\n",
    "import pickle\n",
    "pickle_filepath = \"./tmp/imputed_tensor_train_data.pkl\"\n",
    "# Load the variables back\n",
    "with open(pickle_filepath, \"rb\") as f:\n",
    "    loaded_variables = pickle.load(f)\n",
    "# Verify loaded variables\n",
    "combined_features_train = loaded_variables[\"combined_features\"]\n",
    "unique_features_train = loaded_variables[\"unique_features_combined\"]\n",
    "TargetedFeaturesToEvaluate_combined_train = loaded_variables[\"TargetedFeaturesToEvaluate_combined\"]\n",
    "\n",
    "\n",
    "unique_features = np.sort(parsed_features_complex_df[\"feature_name\"].unique())\n",
    "unique_features#.shape\n",
    "\n",
    "\n",
    "# Find indices of unique_features_train in unique_features\n",
    "unique_features_IntrainSetIndices = [np.where(unique_features == feature)[0][0] for feature in unique_features_train if feature in unique_features]\n",
    "unique_features_use = unique_features[unique_features_IntrainSetIndices]\n",
    "len(unique_features_IntrainSetIndices)\n",
    "\n",
    "# List of features to find\n",
    "target_features = [\n",
    "    \"ENSG00000277632.1\",\n",
    "    \"DMSO_P01579\", \"DMSO_Q16552\", \"DMSO_P05113\", \"PHA_P01579\",\n",
    "    \"PHA_Q16552\", \"PHA_P05113\", \"PT_P01579\", \"PT_Q16552\", \"PT_P05113\",\n",
    "    \"Monocytes\", \"IgG_PT\"\n",
    "]\n",
    "\n",
    "# Find indices of the target features in the unique_features array\n",
    "FeaturesToIncludeIndices = [\n",
    "    np.where(unique_features_use == feature)[0][0] if feature in unique_features_use else -1\n",
    "    for feature in target_features\n",
    "]\n",
    "\n",
    "# Display the results\n",
    "FeaturesToIncludeIndices\n",
    "\n",
    "# Apply the function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tensor_3d_with_assertions, subjects, temporal_dims, features = create_3d_tensor_with_assertions(\n",
    "    combined_dataframe, parsed_features_complex_df, device=device)\n",
    "tensor_3d_with_assertions = tensor_3d_with_assertions[:, :, unique_features_IntrainSetIndices]\n",
    "features = features[unique_features_IntrainSetIndices]\n",
    "# Output the shape of the resulting tensor\n",
    "print(f\"Final tensor shape: {tensor_3d_with_assertions.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# Data_tensor_on_cpu = tensor_3d_no_chunking_vectorized.cpu()\n",
    "Data_tensor_on_cpu = tensor_3d_with_assertions.cpu()\n",
    "Data_tensor_on_cpu.shape\n",
    "\n",
    "variables_to_save = {\n",
    "    \"challenge_tensor\": Data_tensor_on_cpu,\n",
    "    \"features\": features,\n",
    "    \"subject_ids\": subjects,\n",
    "}\n",
    "with open(\"./tmp/tensor_challenge_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(variables_to_save, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the notebook takes previously processed and imputed data and proceeds through a series of steps to fit machine learning models and generate predictions for downstream tasks.\n",
    "\n",
    "#### Overview of the Steps\n",
    "\n",
    "1. **Data Loading and Filtering:**\n",
    "   - Loads a previously saved, fully processed and imputed dataset (`imputed_tensor_train_data.pkl`).\n",
    "   - Extracts the `imputed_tensor`, `combined_features`, and `subject_ids_with_low_features_count`.\n",
    "   - Filters out subjects with insufficient feature coverage and selects only the `combined_features` of interest.  \n",
    "   - The result is a filtered and cleaned PyTorch tensor `X_imputed_filtered` that is free of NaNs (replaced with zeros).\n",
    "\n",
    "2. **Data Normalization:**\n",
    "   - Splits the filtered data into an input slice `X_input` (the first timepoint) and target slices `Y_target` (subsequent timepoints).\n",
    "   - A normalization step is performed across all features and timepoints combined, ensuring that each feature has zero mean and unit variance.  \n",
    "   - The normalization parameters (mean and std) are stored for future use, such as de-normalization of predictions.\n",
    "\n",
    "3. **Model Training Using XGBoost:**\n",
    "   - Each timepoint of interest is modeled separately, allowing for flexible training strategies and comparisons across timepoints.\n",
    "   - Flattened and normalized input features `X_normalized` at T0 serve as predictors, and the normalized values at each target timepoint `Y_normalized[:, t, :]` serve as targets.\n",
    "   - The code uses XGBoost with GPU acceleration (if available) for efficient model training.  \n",
    "   - Models are trained on a train-test split, and performance is measured using RMSE and Adjusted R² metrics.\n",
    "\n",
    "4. **Model Evaluation:**\n",
    "   - For each modeled timepoint, the RMSE and Adjusted R² metrics are computed and printed. This provides insight into how well the model captures the variability of the data and how well it generalizes.\n",
    "\n",
    "5. **Model and Parameters Saving:**\n",
    "   - Trained XGBoost models, evaluation metrics, and parameters are saved into a single pickle file for future use, ensuring reproducibility and easy model loading without retraining.\n",
    "\n",
    "6. **Challenge Data Loading and Prediction:**\n",
    "   - A \"challenge_tensor\" (i.e., a dataset provided for a predictive challenge) is loaded from another pickle file.\n",
    "   - The challenge dataset is normalized using the previously obtained normalization parameters, ensuring consistent scaling with the training data.\n",
    "   \n",
    "   - Predictions for selected features at various timepoints are generated using the trained models.\n",
    "   - A special case (`timepoint = -1`) directly extracts baseline features rather than producing a model prediction.\n",
    "\n",
    "7. **De-normalizing Predictions:**\n",
    "   - The predicted values, originally in normalized space, are de-normalized back into the original scale of the features.\n",
    "   - This allows for interpreting model outputs in terms of the raw biological or clinical units measured originally.\n",
    "\n",
    "8. **Ranking and Fold-Change Computations:**\n",
    "   - Tasks defined by the challenge are addressed by:\n",
    "     - Ranking subjects based on a single feature at a certain timepoint.\n",
    "     - Computing fold-changes between two timepoints for a feature and ranking subjects accordingly.\n",
    "     - Computing ratios of features (e.g., Th1/Th2) and ranking subjects based on these ratios.\n",
    "   \n",
    "   - All these computed rankings and ratio-based metrics are compiled into a final DataFrame, providing a consolidated summary of model-driven insights.\n",
    "\n",
    "9. **Saving Results:**\n",
    "   - The final rankings and fold-change results are saved to a CSV file, which can be submitted to the challenge or used for further analysis.\n",
    "\n",
    "#### Key Outputs:\n",
    "- **`imputed_tensor_train_data.pkl` and related input files:**  \n",
    "  Previously prepared data and associated variables.\n",
    "  \n",
    "- **`model_param_data.pkl` (or similarly named file):**  \n",
    "  Stores trained XGBoost models, their metrics, and parameters for reproducibility and downstream evaluation.\n",
    "  \n",
    "- **`task_rankings.csv`:**  \n",
    "  Final computed rankings of subjects for each defined task, based on model predictions of selected features at given timepoints or fold-changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.13140\n",
      "[1]\tvalidation_0-rmse:0.12281\n",
      "[2]\tvalidation_0-rmse:0.11490\n",
      "[3]\tvalidation_0-rmse:0.10759\n",
      "[4]\tvalidation_0-rmse:0.10087\n",
      "[5]\tvalidation_0-rmse:0.09466\n",
      "[6]\tvalidation_0-rmse:0.08896\n",
      "[7]\tvalidation_0-rmse:0.08370\n",
      "[8]\tvalidation_0-rmse:0.07887\n",
      "[9]\tvalidation_0-rmse:0.07445\n",
      "[10]\tvalidation_0-rmse:0.07042\n",
      "[11]\tvalidation_0-rmse:0.06675\n",
      "[12]\tvalidation_0-rmse:0.06341\n",
      "[13]\tvalidation_0-rmse:0.06037\n",
      "[14]\tvalidation_0-rmse:0.05761\n",
      "[15]\tvalidation_0-rmse:0.05516\n",
      "[16]\tvalidation_0-rmse:0.05292\n",
      "[17]\tvalidation_0-rmse:0.05090\n",
      "[18]\tvalidation_0-rmse:0.04909\n",
      "[19]\tvalidation_0-rmse:0.04750\n",
      "[20]\tvalidation_0-rmse:0.04606\n",
      "[21]\tvalidation_0-rmse:0.04476\n",
      "[22]\tvalidation_0-rmse:0.04360\n",
      "[23]\tvalidation_0-rmse:0.04261\n",
      "[24]\tvalidation_0-rmse:0.04170\n",
      "[25]\tvalidation_0-rmse:0.04088\n",
      "[26]\tvalidation_0-rmse:0.04020\n",
      "[27]\tvalidation_0-rmse:0.03956\n",
      "[28]\tvalidation_0-rmse:0.03900\n",
      "[29]\tvalidation_0-rmse:0.03854\n",
      "[30]\tvalidation_0-rmse:0.03810\n",
      "[31]\tvalidation_0-rmse:0.03772\n",
      "[32]\tvalidation_0-rmse:0.03738\n",
      "[33]\tvalidation_0-rmse:0.03709\n",
      "[34]\tvalidation_0-rmse:0.03681\n",
      "[35]\tvalidation_0-rmse:0.03658\n",
      "[36]\tvalidation_0-rmse:0.03637\n",
      "[37]\tvalidation_0-rmse:0.03620\n",
      "[38]\tvalidation_0-rmse:0.03602\n",
      "[39]\tvalidation_0-rmse:0.03587\n",
      "[40]\tvalidation_0-rmse:0.03573\n",
      "[41]\tvalidation_0-rmse:0.03560\n",
      "[42]\tvalidation_0-rmse:0.03549\n",
      "[43]\tvalidation_0-rmse:0.03539\n",
      "[44]\tvalidation_0-rmse:0.03531\n",
      "[45]\tvalidation_0-rmse:0.03523\n",
      "[46]\tvalidation_0-rmse:0.03516\n",
      "[47]\tvalidation_0-rmse:0.03510\n",
      "[48]\tvalidation_0-rmse:0.03505\n",
      "[49]\tvalidation_0-rmse:0.03500\n",
      "[50]\tvalidation_0-rmse:0.03496\n",
      "[51]\tvalidation_0-rmse:0.03492\n",
      "[52]\tvalidation_0-rmse:0.03488\n",
      "[53]\tvalidation_0-rmse:0.03485\n",
      "[54]\tvalidation_0-rmse:0.03483\n",
      "[55]\tvalidation_0-rmse:0.03481\n",
      "[56]\tvalidation_0-rmse:0.03479\n",
      "[57]\tvalidation_0-rmse:0.03479\n",
      "[58]\tvalidation_0-rmse:0.03479\n",
      "[0]\tvalidation_0-rmse:0.13001\n",
      "[1]\tvalidation_0-rmse:0.12145\n",
      "[2]\tvalidation_0-rmse:0.11352\n",
      "[3]\tvalidation_0-rmse:0.10622\n",
      "[4]\tvalidation_0-rmse:0.09949\n",
      "[5]\tvalidation_0-rmse:0.09328\n",
      "[6]\tvalidation_0-rmse:0.08759\n",
      "[7]\tvalidation_0-rmse:0.08234\n",
      "[8]\tvalidation_0-rmse:0.07754\n",
      "[9]\tvalidation_0-rmse:0.07313\n",
      "[10]\tvalidation_0-rmse:0.06911\n",
      "[11]\tvalidation_0-rmse:0.06544\n",
      "[12]\tvalidation_0-rmse:0.06212\n",
      "[13]\tvalidation_0-rmse:0.05909\n",
      "[14]\tvalidation_0-rmse:0.05634\n",
      "[15]\tvalidation_0-rmse:0.05386\n",
      "[16]\tvalidation_0-rmse:0.05163\n",
      "[17]\tvalidation_0-rmse:0.04962\n",
      "[18]\tvalidation_0-rmse:0.04783\n",
      "[19]\tvalidation_0-rmse:0.04622\n",
      "[20]\tvalidation_0-rmse:0.04477\n",
      "[21]\tvalidation_0-rmse:0.04349\n",
      "[22]\tvalidation_0-rmse:0.04234\n",
      "[23]\tvalidation_0-rmse:0.04133\n",
      "[24]\tvalidation_0-rmse:0.04044\n",
      "[25]\tvalidation_0-rmse:0.03963\n",
      "[26]\tvalidation_0-rmse:0.03892\n",
      "[27]\tvalidation_0-rmse:0.03830\n",
      "[28]\tvalidation_0-rmse:0.03776\n",
      "[29]\tvalidation_0-rmse:0.03727\n",
      "[30]\tvalidation_0-rmse:0.03685\n",
      "[31]\tvalidation_0-rmse:0.03648\n",
      "[32]\tvalidation_0-rmse:0.03616\n",
      "[33]\tvalidation_0-rmse:0.03587\n",
      "[34]\tvalidation_0-rmse:0.03563\n",
      "[35]\tvalidation_0-rmse:0.03542\n",
      "[36]\tvalidation_0-rmse:0.03524\n",
      "[37]\tvalidation_0-rmse:0.03505\n",
      "[38]\tvalidation_0-rmse:0.03488\n",
      "[39]\tvalidation_0-rmse:0.03472\n",
      "[40]\tvalidation_0-rmse:0.03458\n",
      "[41]\tvalidation_0-rmse:0.03445\n",
      "[42]\tvalidation_0-rmse:0.03434\n",
      "[43]\tvalidation_0-rmse:0.03425\n",
      "[44]\tvalidation_0-rmse:0.03416\n",
      "[45]\tvalidation_0-rmse:0.03408\n",
      "[46]\tvalidation_0-rmse:0.03402\n",
      "[47]\tvalidation_0-rmse:0.03396\n",
      "[48]\tvalidation_0-rmse:0.03390\n",
      "[49]\tvalidation_0-rmse:0.03386\n",
      "[50]\tvalidation_0-rmse:0.03382\n",
      "[51]\tvalidation_0-rmse:0.03378\n",
      "[52]\tvalidation_0-rmse:0.03374\n",
      "[53]\tvalidation_0-rmse:0.03371\n",
      "[54]\tvalidation_0-rmse:0.03369\n",
      "[55]\tvalidation_0-rmse:0.03366\n",
      "[56]\tvalidation_0-rmse:0.03364\n",
      "[57]\tvalidation_0-rmse:0.03364\n",
      "[58]\tvalidation_0-rmse:0.03364\n",
      "[59]\tvalidation_0-rmse:0.03366\n",
      "[0]\tvalidation_0-rmse:0.12917\n",
      "[1]\tvalidation_0-rmse:0.12069\n",
      "[2]\tvalidation_0-rmse:0.11288\n",
      "[3]\tvalidation_0-rmse:0.10570\n",
      "[4]\tvalidation_0-rmse:0.09915\n",
      "[5]\tvalidation_0-rmse:0.09308\n",
      "[6]\tvalidation_0-rmse:0.08751\n",
      "[7]\tvalidation_0-rmse:0.08242\n",
      "[8]\tvalidation_0-rmse:0.07781\n",
      "[9]\tvalidation_0-rmse:0.07357\n",
      "[10]\tvalidation_0-rmse:0.06973\n",
      "[11]\tvalidation_0-rmse:0.06624\n",
      "[12]\tvalidation_0-rmse:0.06311\n",
      "[13]\tvalidation_0-rmse:0.06029\n",
      "[14]\tvalidation_0-rmse:0.05773\n",
      "[15]\tvalidation_0-rmse:0.05548\n",
      "[16]\tvalidation_0-rmse:0.05344\n",
      "[17]\tvalidation_0-rmse:0.05162\n",
      "[18]\tvalidation_0-rmse:0.05000\n",
      "[19]\tvalidation_0-rmse:0.04856\n",
      "[20]\tvalidation_0-rmse:0.04726\n",
      "[21]\tvalidation_0-rmse:0.04614\n",
      "[22]\tvalidation_0-rmse:0.04517\n",
      "[23]\tvalidation_0-rmse:0.04429\n",
      "[24]\tvalidation_0-rmse:0.04358\n",
      "[25]\tvalidation_0-rmse:0.04291\n",
      "[26]\tvalidation_0-rmse:0.04234\n",
      "[27]\tvalidation_0-rmse:0.04186\n",
      "[28]\tvalidation_0-rmse:0.04147\n",
      "[29]\tvalidation_0-rmse:0.04112\n",
      "[30]\tvalidation_0-rmse:0.04084\n",
      "[31]\tvalidation_0-rmse:0.04058\n",
      "[32]\tvalidation_0-rmse:0.04039\n",
      "[33]\tvalidation_0-rmse:0.04022\n",
      "[34]\tvalidation_0-rmse:0.04003\n",
      "[35]\tvalidation_0-rmse:0.03989\n",
      "[36]\tvalidation_0-rmse:0.03979\n",
      "[37]\tvalidation_0-rmse:0.03968\n",
      "[38]\tvalidation_0-rmse:0.03961\n",
      "[39]\tvalidation_0-rmse:0.03953\n",
      "[40]\tvalidation_0-rmse:0.03949\n",
      "[41]\tvalidation_0-rmse:0.03946\n",
      "[42]\tvalidation_0-rmse:0.03941\n",
      "[43]\tvalidation_0-rmse:0.03941\n",
      "[44]\tvalidation_0-rmse:0.03938\n",
      "[45]\tvalidation_0-rmse:0.03938\n",
      "[46]\tvalidation_0-rmse:0.03939\n",
      "[0]\tvalidation_0-rmse:0.35625\n",
      "[1]\tvalidation_0-rmse:0.35514\n",
      "[2]\tvalidation_0-rmse:0.35448\n",
      "[3]\tvalidation_0-rmse:0.35416\n",
      "[4]\tvalidation_0-rmse:0.35416\n",
      "[5]\tvalidation_0-rmse:0.35429\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cupy as cp\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Load processed data and variables\n",
    "pickle_filepath = \"./tmp/imputed_tensor_train_data.pkl\"\n",
    "with open(pickle_filepath, \"rb\") as f:\n",
    "    loaded_variables = pickle.load(f)\n",
    "\n",
    "imputed_tensor = loaded_variables[\"imputed_tensor\"]\n",
    "combined_features = loaded_variables[\"combined_features\"]\n",
    "subject_ids_with_low_features_count = loaded_variables[\"subject_ids_with_low_features_count\"]\n",
    "TargetedFeaturesToEvaluate_combined = loaded_variables[\"TargetedFeaturesToEvaluate_combined\"]\n",
    "\n",
    "X_imputed = imputed_tensor[:, :, combined_features]\n",
    "all_subject_indices = torch.arange(X_imputed.size(0))\n",
    "valid_subject_indices = [idx for idx in all_subject_indices if idx not in subject_ids_with_low_features_count]\n",
    "X_imputed_filtered = X_imputed[valid_subject_indices]\n",
    "X_imputed_filtered = torch.nan_to_num(X_imputed_filtered, nan=0.0)\n",
    "\n",
    "X_input = X_imputed_filtered[:, 0, :].unsqueeze(1)\n",
    "Y_target = X_imputed_filtered[:, 1:, :]\n",
    "\n",
    "def normalize_3d(X, Y):\n",
    "    combined = np.concatenate([X, Y], axis=1)\n",
    "    feature_mean = np.mean(combined, axis=(0, 1), keepdims=True)\n",
    "    feature_std = np.std(combined, axis=(0, 1), keepdims=True)\n",
    "    normalized = (combined - feature_mean) / (feature_std + 1e-8)\n",
    "    X_normalized = normalized[:, :1, :]\n",
    "    Y_normalized = normalized[:, 1:, :]\n",
    "    normalization_params = {\"mean\": feature_mean, \"std\": feature_std}\n",
    "    return X_normalized, Y_normalized, normalization_params\n",
    "\n",
    "X_normalized, Y_normalized, normalization_params = normalize_3d(X_input.numpy(), Y_target.numpy())\n",
    "\n",
    "def adjusted_r2_from_rmse(rmse, y_true, n_features):\n",
    "    n = len(y_true)\n",
    "    variance_y = np.var(y_true)\n",
    "    ss_total = variance_y * n\n",
    "    ss_residual = (rmse ** 2) * n\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - n_features - 1))\n",
    "    return adjusted_r2\n",
    "\n",
    "def flatten_Y(Y):\n",
    "    num_samples, seq_len, output_dim = Y.shape\n",
    "    return Y.reshape(num_samples, seq_len * output_dim)\n",
    "\n",
    "timepoints = [0, 2, 9, 15]\n",
    "models = {}\n",
    "metrics = {}\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "for t in timepoints:\n",
    "    print(f\"Training model for time point {t}...\")\n",
    "    Y_t = Y_normalized[:, t, :]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_normalized.squeeze(1),\n",
    "        Y_t,\n",
    "        test_size=0.2,\n",
    "        random_state=seed\n",
    "    )\n",
    "    X_train = cp.array(X_train)\n",
    "    X_test = cp.array(X_test)\n",
    "    Y_train = cp.array(Y_train)\n",
    "    Y_test = cp.array(Y_test)\n",
    "\n",
    "    params = {\n",
    "        # Param_1\n",
    "        'n_estimators': 1272,\n",
    "        'max_depth': 100,\n",
    "        'max_leaves': 0,\n",
    "        'learning_rate': 0.0753493322967014,\n",
    "        'max_delta_step': 4,\n",
    "        'lambda': 4,\n",
    "        'subsample': 1,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'early_stopping_rounds': 2,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 2,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'rmse',\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "\n",
    "        # # Param_2\n",
    "        # 'n_estimators': 318000,\n",
    "        # 'max_depth': 500,\n",
    "        # 'max_leaves': 0,\n",
    "        # 'learning_rate': 0.0753493322967014,\n",
    "        # 'max_delta_step': 4,\n",
    "        # 'alpha': 1.230710427888436,\n",
    "        # 'subsample': 1,\n",
    "        # 'objective': 'reg:squarederror',\n",
    "        # 'early_stopping_rounds': 1,\n",
    "        # 'n_jobs': -1,\n",
    "        # 'verbosity': 2,\n",
    "        # 'tree_method': 'hist',\n",
    "        # 'random_state': 42,\n",
    "        # 'eval_metric': 'rmse',\n",
    "        # 'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "\n",
    "        # # Param_3\n",
    "        # 'n_estimators': 318,\n",
    "        # 'max_depth': 50,\n",
    "        # 'max_leaves': 0,\n",
    "        # 'learning_rate': 0.0753493322967014,\n",
    "        # 'max_delta_step': 4,\n",
    "        # 'alpha': 1.230710427888436,\n",
    "        # 'subsample': 1,\n",
    "        # 'objective': 'reg:squarederror',\n",
    "        # 'early_stopping_rounds': 2,\n",
    "        # 'n_jobs': -1,\n",
    "        # 'verbosity': 2,\n",
    "        # 'tree_method': 'hist',\n",
    "        # 'random_state': seed,\n",
    "        # 'eval_metric': \"rmse\",\n",
    "        # 'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, Y_train, eval_set=[(X_test, Y_test)], verbose=True)\n",
    "    models[t] = model\n",
    "\n",
    "    Y_test_pred = model.predict(X_test)\n",
    "    test_rmse = root_mean_squared_error(Y_test.get(), Y_test_pred)\n",
    "    adjusted_r2 = adjusted_r2_from_rmse(test_rmse, Y_test.get(), X_train.shape[1])\n",
    "    metrics[t] = {'rmse': test_rmse, 'adjusted_r2': adjusted_r2}\n",
    "# Print summary of metrics\n",
    "for t, metric in metrics.items():\n",
    "    print(f\"Time Point {t}: Test RMSE = {metric['rmse']:.4f}, Adjusted R^2 = {metric['adjusted_r2']:.4f}\")\n",
    "\n",
    "save_data = {\n",
    "    \"models\": models,\n",
    "    \"metrics\": metrics,\n",
    "    \"params\": params\n",
    "}\n",
    "\n",
    "save_file = \"model_param_data_1.pkl\"\n",
    "# save_file = \"model_param_data_2.pkl\"\n",
    "# save_file = \"model_param_data_3.pkl\"\n",
    "\n",
    "with open(save_file, \"wb\") as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Challenge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/ssbio/aosinuga2/mytorchC/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pickle_filepath = \"./tmp/tensor_challenge_data.pkl\"\n",
    "with open(pickle_filepath, \"rb\") as f:\n",
    "    loaded_variables = pickle.load(f)\n",
    "\n",
    "challenge_tensor = loaded_variables[\"challenge_tensor\"]\n",
    "challenge_features = loaded_variables[\"features\"]\n",
    "challenge_subjects = loaded_variables[\"subject_ids\"]\n",
    "\n",
    "def denormalize_3d(data, params):\n",
    "    mean = params[\"mean\"]\n",
    "    std = params[\"std\"]\n",
    "    return data * std + mean\n",
    "\n",
    "def normalize_new_3d(X, params):\n",
    "    mean = params[\"mean\"]\n",
    "    std = params[\"std\"]\n",
    "    normalized = (X - mean) / (std + 1e-8)\n",
    "    return normalized\n",
    "\n",
    "def predict_and_save_features_with_last_timepoint(X_normalized, selected_features, models, timepoints, subjects):\n",
    "    results = []\n",
    "    for t in timepoints:\n",
    "        if t == -1:\n",
    "            for feature_idx in selected_features:\n",
    "                feature_values = X_normalized[:, feature_idx]\n",
    "                for sample_idx, value in enumerate(feature_values):\n",
    "                    results.append({\n",
    "                        \"Timepoint\": t,\n",
    "                        \"Feature_Index\": feature_idx,\n",
    "                        \"Sample_Index\": subjects[sample_idx],\n",
    "                        \"Predicted_Value\": value.cpu().numpy() if torch.is_tensor(value) else value\n",
    "                    })\n",
    "        else:\n",
    "            model = models[t]\n",
    "            Y_pred = model.predict(X_normalized)\n",
    "            for feature_idx in selected_features:\n",
    "                feature_values = Y_pred[:, feature_idx]\n",
    "                for sample_idx, value in enumerate(feature_values):\n",
    "                    results.append({\n",
    "                        \"Timepoint\": t,\n",
    "                        \"Feature_Index\": feature_idx,\n",
    "                        \"Sample_Index\": subjects[sample_idx],\n",
    "                        \"Predicted_Value\": value\n",
    "                    })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "X_normalized_new = normalize_new_3d(challenge_tensor, normalization_params).squeeze(1)\n",
    "selected_features = [6643, 6645, 6607, 6679, 6680]\n",
    "timepoints = [0, 2, 9, 15, -1]\n",
    "predicted_features_df = predict_and_save_features_with_last_timepoint(X_normalized_new, selected_features, models, timepoints, challenge_subjects)\n",
    "\n",
    "def denormalize_predictions(predictions_df, normalization_params):\n",
    "    predictions_df[\"De_Normalized_Value\"] = predictions_df.apply(\n",
    "        lambda row: row[\"Predicted_Value\"] * normalization_params[\"std\"][0, 0, row[\"Feature_Index\"]] +\n",
    "                    normalization_params[\"mean\"][0, 0, row[\"Feature_Index\"]], axis=1\n",
    "    )\n",
    "    return predictions_df\n",
    "\n",
    "predicted_features_df = denormalize_predictions(predicted_features_df, normalization_params)\n",
    "\n",
    "tasks = {\n",
    "    \"1.1\": {\"feature\": 6643, \"timepoint\": 9, \"type\": \"rank\"},\n",
    "    \"1.2\": {\"feature\": 6643, \"timepoint_1\": 9, \"timepoint_0\": -1, \"type\": \"fold_change\"},\n",
    "    \"2.1\": {\"feature\": 6645, \"timepoint\": 0, \"type\": \"rank\"},\n",
    "    \"2.2\": {\"feature\": 6645, \"timepoint_1\": 0, \"timepoint_0\": -1, \"type\": \"fold_change\"},\n",
    "    \"3.1\": {\"feature\": 6607, \"timepoint\": 2, \"type\": \"rank\"},\n",
    "    \"3.2\": {\"feature\": 6607, \"timepoint_1\": 2, \"timepoint_0\": -1, \"type\": \"fold_change\"},\n",
    "    \"4.1\": {\"Th1\": 6679, \"Th2\": 6680, \"timepoint\": 15, \"type\": \"ratio\"},\n",
    "}\n",
    "\n",
    "results = {\"SubjectID\": challenge_subjects}\n",
    "\n",
    "for task, details in tasks.items():\n",
    "    if details[\"type\"] == \"rank\":\n",
    "        feature_values = predicted_features_df[\n",
    "            (predicted_features_df[\"Feature_Index\"] == details[\"feature\"]) &\n",
    "            (predicted_features_df[\"Timepoint\"] == details[\"timepoint\"])\n",
    "        ].sort_values(by=\"Predicted_Value\", ascending=False)\n",
    "        results[task] = feature_values[\"Predicted_Value\"].rank(ascending=False).values\n",
    "    elif details[\"type\"] == \"fold_change\":\n",
    "        timepoint_1_values = predicted_features_df[\n",
    "            (predicted_features_df[\"Feature_Index\"] == details[\"feature\"]) &\n",
    "            (predicted_features_df[\"Timepoint\"] == details[\"timepoint_1\"])\n",
    "        ][\"Predicted_Value\"].values\n",
    "        timepoint_0_values = predicted_features_df[\n",
    "            (predicted_features_df[\"Feature_Index\"] == details[\"feature\"]) &\n",
    "            (predicted_features_df[\"Timepoint\"] == details[\"timepoint_0\"])\n",
    "        ][\"Predicted_Value\"].values\n",
    "        fold_changes = timepoint_1_values / timepoint_0_values\n",
    "        results[task] = pd.Series(fold_changes).rank(ascending=False).values\n",
    "    elif details[\"type\"] == \"ratio\":\n",
    "        Th1_values = predicted_features_df[\n",
    "            (predicted_features_df[\"Feature_Index\"] == details[\"Th1\"]) &\n",
    "            (predicted_features_df[\"Timepoint\"] == details[\"timepoint\"])\n",
    "        ][\"Predicted_Value\"].values\n",
    "        Th2_values = predicted_features_df[\n",
    "            (predicted_features_df[\"Feature_Index\"] == details[\"Th2\"]) &\n",
    "            (predicted_features_df[\"Timepoint\"] == details[\"timepoint\"])\n",
    "        ][\"Predicted_Value\"].values\n",
    "        ratios = Th1_values / Th2_values\n",
    "        results[task] = pd.Series(ratios).rank(ascending=False).values\n",
    "\n",
    "final_results_df = pd.DataFrame(results)\n",
    "output_filepath = \"tasks_rankings_1.csv\"\n",
    "# output_filepath = \"tasks_rankings_2.csv\"\n",
    "# output_filepath = \"tasks_rankings_3.csv\"\n",
    "\n",
    "final_results_df.to_csv(output_filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
